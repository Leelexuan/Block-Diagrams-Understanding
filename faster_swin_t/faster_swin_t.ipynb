{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CBD dataset (for training)\n",
    "\n",
    "### Transformation\n",
    " - resize to 244,244\n",
    " - normalized based on calculations from training set\n",
    " - converted to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dataset')\n",
    "from dataset_parser import XMLDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.9362358181861065, 0.9420508144749817, 0.9394349808120092]\n",
      "Std: [0.16755679634617304, 0.15510376581525342, 0.1576581799256157]\n"
     ]
    }
   ],
   "source": [
    "#calculation mean and std of training set for normalization\n",
    "\n",
    "def compute_dataset_stats(image_dir):\n",
    "    mean = np.zeros(3)\n",
    "    std = np.zeros(3)\n",
    "    total_pixels = 0\n",
    "\n",
    "    # List all files in the directory\n",
    "    image_filenames = sorted(os.listdir(image_dir))\n",
    "\n",
    "    for img_filename in image_filenames:\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "\n",
    "        # Open the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image) / 255.0  # Convert to range [0, 1]\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean += image.mean(axis=(0, 1))\n",
    "        std += image.std(axis=(0, 1))\n",
    "        total_pixels += 1\n",
    "\n",
    "    # Compute average mean and std across all images\n",
    "    mean /= total_pixels\n",
    "    std /= total_pixels\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "# Example usage\n",
    "image_dir = '../dataset/train_img'\n",
    "mean, std = compute_dataset_stats(image_dir)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "#apply transformations to image and bounding boxes\n",
    "\n",
    "transform = v2.Compose([\n",
    "    #scales and converst to float32 tensors\n",
    "    v2.ToTensor()\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],\n",
      "\n",
      "        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],\n",
      "\n",
      "        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]]]), {'boxes': tensor([[ 201.,   31.,  412.,  123.],\n",
      "        [ 720.,  226., 1184.,  431.],\n",
      "        [ 720.,  534., 1176.,  776.],\n",
      "        [ 798., 1220., 1015., 1373.],\n",
      "        [ 103.,  187.,  506.,  456.],\n",
      "        [  95.,  515.,  512.,  790.],\n",
      "        [ 106.,  859.,  501., 1104.],\n",
      "        [  90., 1165.,  517., 1434.],\n",
      "        [  92., 1495.,  526., 1768.],\n",
      "        [ 267.,   62.,  348.,  101.],\n",
      "        [ 581.,  306.,  640.,  345.],\n",
      "        [ 809.,  304., 1084.,  354.],\n",
      "        [ 765.,  573., 1128.,  623.],\n",
      "        [ 817., 1276.,  990., 1315.],\n",
      "        [ 140.,  604.,  470.,  679.],\n",
      "        [ 134.,  229.,  470.,  351.],\n",
      "        [ 212.,  934.,  431., 1015.],\n",
      "        [ 142., 1254.,  470., 1356.],\n",
      "        [ 120., 1584.,  487., 1656.],\n",
      "        [ 287.,  112.,  323.,  201.],\n",
      "        [ 292.,  451.,  320.,  531.],\n",
      "        [ 606.,  640.,  731.,  668.],\n",
      "        [ 290.,  770.,  320.,  868.],\n",
      "        [ 290., 1090.,  317., 1176.],\n",
      "        [ 290., 1418.,  320., 1506.],\n",
      "        [ 501., 1368.,  920., 1640.],\n",
      "        [ 487.,  770.,  970.,  993.]]), 'labels': tensor([7, 6, 6, 6, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2]), 'image_id': tensor([7, 6, 6, 6, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2]), 'area': tensor([7, 6, 6, 6, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['boxes'] = torch.tensor(target['boxes'], dtype=torch.float32)\n",
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['image_id'] = torch.tensor(target['labels'], dtype=torch.int64)\n",
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['area'] = torch.tensor(target['labels'], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    \"text\" : 1,\n",
    "    \"arrow\" : 2,\n",
    "    \"connection\" : 3,\n",
    "    \"data\": 4,\n",
    "    \"decision\": 5,\n",
    "    \"process\" : 6,\n",
    "    \"terminator\" : 7\n",
    "}\n",
    "\n",
    "train = XMLDataset(image_dir='../dataset/train_img', annotation_dir='../dataset/xml_files/train', label_map=label_map, transform=transform)\n",
    "test = XMLDataset(image_dir='../dataset/test_img', annotation_dir='../dataset/xml_files/test', label_map=label_map, transform=transform)\n",
    "validation = XMLDataset(image_dir='../dataset/val_img', annotation_dir='../dataset/xml_files/val', label_map=label_map, transform=transform)\n",
    "data = train[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertions passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# assert data['objects']['bbox'][0].tolist() == [201., 31., 211., 92.], \"First box coordinates do not match\"\n",
    "(image, target) = data\n",
    "# assert target['labels'][:5] == [7, 6, 6, 6, 5], \"First five labels do not match\"\n",
    "assert len(target['area']) == len(target['labels']), \"Length of area and category do not match\"\n",
    "assert len(target['boxes']) == len(target['area']), \"Length of bbox and area do not match\"\n",
    "# print(\"new image size\", image.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Assertions passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FCA dataset (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomSwinBackbone(\n",
      "  (backbone): Swinv2Backbone(\n",
      "    (embeddings): Swinv2Embeddings(\n",
      "      (patch_embeddings): Swinv2PatchEmbeddings(\n",
      "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Swinv2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=False)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Identity()\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=False)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.00909090880304575)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.0181818176060915)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.027272727340459824)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.036363635212183)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.045454543083906174)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.054545458406209946)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.06363636255264282)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.0727272778749466)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.08181818574666977)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.09090909361839294)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.10000000149011612)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, Swinv2Backbone, AutoModel, Swinv2Config, AutoBackbone\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# config = Swinv2Config()\n",
    "# config.out_features = [\"stage2\", \"stage3\", \"stage4\"]\n",
    "# swin_backbone = AutoModel.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "swin_backbone = AutoBackbone.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\", out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "\n",
    "# print(config)\n",
    "\n",
    "\n",
    "class CustomSwinBackbone(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone\n",
    "        out = self.backbone(x, output_hidden_states=False, output_attentions=False)\n",
    "        feature_map = out.feature_maps\n",
    "        # print(len(out.feature_maps))\n",
    "        out_channel = 768\n",
    "        feature_dict = {}\n",
    "        \n",
    "        for i in range(len(feature_map)):\n",
    "            in_channel = list(out.feature_maps[i].shape)[1]\n",
    "            conv = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1)\n",
    "            feature_dict[str(i)] = conv(feature_map[i])\n",
    "            \n",
    "        # print(feature_dict[\"0\"].shape)\n",
    "        # print(feature_dict[\"1\"].shape)\n",
    "        # print(feature_dict[\"2\"].shape)\n",
    "        # print(feature_dict[\"3\"].shape)\n",
    "\n",
    "        # Permute the output to (b, c, h, w)\n",
    "        # out[0] corresponds to the feature map (assuming the backbone outputs a list of feature maps)\n",
    "        # out[0].shape is (b, h, w, c), and we need to permute it to (b, c, h, w)\n",
    "        return feature_dict\n",
    "# Create the custom backbone with the permute operation\n",
    "swin_backbone = CustomSwinBackbone(swin_backbone)\n",
    "print(swin_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision\n",
    "\n",
    "swin_backbone.out_channels = 768\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),\n",
    "           (32, 64, 128, 256, 512),\n",
    "           (32, 64, 128, 256, 512),\n",
    "           (32, 64, 128, 256, 512)), \n",
    "    aspect_ratios=((0.5, 1.0, 2.0),\n",
    "                   (0.5, 1.0, 2.0),\n",
    "                   (0.5, 1.0, 2.0),\n",
    "                   (0.5, 1.0, 2.0)))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=[\"1\", \"2\", \"3\"], \n",
    "    output_size = 7, \n",
    "    sampling_ratio=2\n",
    "    )\n",
    "\n",
    "model = FasterRCNN(\n",
    "    backbone=swin_backbone,\n",
    "    #7 + 1 for background\n",
    "    num_classes=8,\n",
    "    # min_size=256,\n",
    "    max_size=256,\n",
    "    # image_mean=mean,\n",
    "    # image_std=std,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9362358181861065, 0.9420508144749817, 0.9394349808120092] [0.16755679634617304, 0.15510376581525342, 0.1576581799256157]\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # collated = []\n",
    "    # for item in batch:\n",
    "    #     images = item[0]\n",
    "    #     labels = item[1]\n",
    "    #     images = images.to(device)\n",
    "    #     for key, value in labels.items():\n",
    "    #         if torch.is_tensor(value):\n",
    "    #             labels[key] = value.to(device) \n",
    "\n",
    "    #     collated.append((images, labels))\n",
    "    \n",
    "    return tuple(zip(*batch))\n",
    "            \n",
    "# train_subset = Subset(train, list(range(8)))\n",
    "# val_subset = Subset(validation, list(range(8)))\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    validation,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edited from pytorch documentation\n",
    "def train_one_epoch(model, optimizer, data_loader,  epoch, lr_scheduler):\n",
    "    try: \n",
    "        model.train()\n",
    "        header = f\"Epoch: [{epoch}]\"\n",
    "        batch_loss = 0\n",
    "        last_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(data_loader):\n",
    "            images, labels = data\n",
    "            \n",
    "            loss_dict = model(images, labels)\n",
    "            # print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # Gather data and report\n",
    "            batch_loss += losses.item()\n",
    "            \n",
    "            last_loss = batch_loss\n",
    "            print(header)\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            batch_loss = 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Print the error and the image_id that caused it\n",
    "        # print(f\"Error for image name {labels['image_id']}\")\n",
    "        # print(data)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        # You can return None or raise the error depending on your need\n",
    "        raise e\n",
    "        \n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
    "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "\n",
    "def eval_forward(model, images, targets):\n",
    "    # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images (list[Tensor]): images to be processed\n",
    "        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "    Returns:\n",
    "        result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "            It returns list[BoxList] contains additional fields\n",
    "            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in images:\n",
    "        val = img.shape[-2:]\n",
    "        assert len(val) == 2\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "    images, targets = model.transform(images, targets)\n",
    "\n",
    "    # Check for degenerate boxes\n",
    "    # TODO: Move this to a function\n",
    "    if targets is not None:\n",
    "        for target_idx, target in enumerate(targets):\n",
    "            boxes = target[\"boxes\"]\n",
    "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "            if degenerate_boxes.any():\n",
    "                # print the first degenerate box\n",
    "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                raise ValueError(\n",
    "                    \"All bounding boxes should have positive height and width.\"\n",
    "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                )\n",
    "\n",
    "    features = model.backbone(images.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = OrderedDict([(\"0\", features)])\n",
    "    model.rpn.training=True\n",
    "    #model.roi_heads.training=True\n",
    "\n",
    "\n",
    "    #####proposals, proposal_losses = model.rpn(images, features, targets)\n",
    "    features_rpn = list(features.values())\n",
    "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
    "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
    "\n",
    "    num_images = len(anchors)\n",
    "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "    # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "    # the proposals\n",
    "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "    proposals = proposals.view(num_images, -1, 4)\n",
    "    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "    proposal_losses = {}\n",
    "    assert targets is not None\n",
    "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
    "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
    "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
    "        objectness, pred_bbox_deltas, labels, regression_targets\n",
    "    )\n",
    "    proposal_losses = {\n",
    "        \"loss_objectness\": loss_objectness,\n",
    "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "    }\n",
    "\n",
    "    #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "    image_shapes = images.image_sizes\n",
    "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
    "    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "    box_features = model.roi_heads.box_head(box_features)\n",
    "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
    "\n",
    "    result: List[Dict[str, torch.Tensor]] = []\n",
    "    detector_losses = {}\n",
    "    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
    "    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
    "    num_images = len(boxes)\n",
    "    for i in range(num_images):\n",
    "        result.append(\n",
    "            {\n",
    "                \"boxes\": boxes[i],\n",
    "                \"labels\": labels[i],\n",
    "                \"scores\": scores[i],\n",
    "            }\n",
    "        )\n",
    "    detections = result\n",
    "    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "    model.rpn.training=False\n",
    "    model.roi_heads.training=False\n",
    "    losses = {}\n",
    "    losses.update(detector_losses)\n",
    "    losses.update(proposal_losses)\n",
    "    return losses, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['boxes'] = torch.tensor(target['boxes'], dtype=torch.float32)\n",
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['image_id'] = torch.tensor(target['labels'], dtype=torch.int64)\n",
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['area'] = torch.tensor(target['labels'], dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]\n",
      "  batch 1 loss: 3.3591132164001465\n",
      "Epoch: [0]\n",
      "  batch 2 loss: 2.997565269470215\n",
      "////////////////////////////////////////////////////////////////////////////////\n",
      "LOSS train 2.997565269470215 valid 3.1706771850585938\n",
      "MAP: 0.0007456097519025207\n",
      "map per class: tensor([ 0.0000,  0.0000, -0.5000,  0.0045,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "import datetime\n",
    "import torchvision\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "# move model to cuda\n",
    "# model.to(device)\n",
    "# model.backbone.to(device) \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train()\n",
    "    avg_loss = train_one_epoch(\n",
    "        epoch=epoch_number,\n",
    "        model=model,\n",
    "        data_loader=data_loader,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "    )\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "    total_map = 0\n",
    "    total_map_per_class = torch.zeros(7)\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        total_predictions = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i, vdata in enumerate(data_loader_val):\n",
    "            vinputs, vlabels = vdata  # Images and their ground-truth labels\n",
    "\n",
    "            # Compute the model outputs and loss\n",
    "            loss_dict, detections = eval_forward(model, vinputs, vlabels)  # Pass inputs and targets\n",
    "            # print(loss_dict)\n",
    "            vloss = sum(loss for loss in loss_dict.values())\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            metric = MeanAveragePrecision(iou_type=\"bbox\", class_metrics=True)\n",
    "            metric.update(detections, vlabels)\n",
    "            map_dict = metric.compute()\n",
    "            # print(map_dict)\n",
    "            total_map += map_dict[\"map\"]\n",
    "            map_per_class = map_dict[\"map_per_class\"]\n",
    "            classes = map_dict[\"classes\"]\n",
    "            \n",
    "            for index, value in zip(classes, map_per_class):\n",
    "                total_map_per_class[index-1] += value\n",
    "            \n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        avg_map = total_map/ (i+1)\n",
    "        avg_map_per_class = total_map_per_class / (i+1)\n",
    "        \n",
    "        print(\"////////////////////////////////////////////////////////////////////////////////\")\n",
    "        print(f\"LOSS train {avg_loss} valid {avg_vloss}\")\n",
    "        print(f\"MAP: {avg_map}\")\n",
    "        print(f\"map per class: {avg_map_per_class}\")\n",
    "        \n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = \"model_{}_{}\".format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
