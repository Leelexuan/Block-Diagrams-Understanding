{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CBD dataset (for training)\n",
    "\n",
    "### Transformation\n",
    " - resize to 244,244\n",
    " - normalized based on calculations from training set\n",
    " - converted to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../dataset')\n",
    "from dataset_parser import XMLDataset\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.9362358181861065, 0.9420508144749817, 0.9394349808120092]\n",
      "Std: [0.16755679634617304, 0.15510376581525342, 0.1576581799256157]\n"
     ]
    }
   ],
   "source": [
    "#calculation mean and std of training set for normalization\n",
    "\n",
    "def compute_dataset_stats(image_dir):\n",
    "    mean = np.zeros(3)\n",
    "    std = np.zeros(3)\n",
    "    total_pixels = 0\n",
    "\n",
    "    # List all files in the directory\n",
    "    image_filenames = sorted(os.listdir(image_dir))\n",
    "\n",
    "    for img_filename in image_filenames:\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "\n",
    "        # Open the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image) / 255.0  # Convert to range [0, 1]\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean += image.mean(axis=(0, 1))\n",
    "        std += image.std(axis=(0, 1))\n",
    "        total_pixels += 1\n",
    "\n",
    "    # Compute average mean and std across all images\n",
    "    mean /= total_pixels\n",
    "    std /= total_pixels\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "# Example usage\n",
    "image_dir = '../dataset/train_img'\n",
    "mean, std = compute_dataset_stats(image_dir)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "#apply transformations to image and bounding boxes\n",
    "\n",
    "transform = v2.Compose([\n",
    "    #scales and converst to float32 tensors\n",
    "    v2.ToTensor()\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],\n",
      "\n",
      "        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]],\n",
      "\n",
      "        [[0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9490, 0.9490, 0.9490]]]), {'boxes': tensor([[ 201.,   31.,  412.,  123.],\n",
      "        [ 720.,  226., 1184.,  431.],\n",
      "        [ 720.,  534., 1176.,  776.],\n",
      "        [ 798., 1220., 1015., 1373.],\n",
      "        [ 103.,  187.,  506.,  456.],\n",
      "        [  95.,  515.,  512.,  790.],\n",
      "        [ 106.,  859.,  501., 1104.],\n",
      "        [  90., 1165.,  517., 1434.],\n",
      "        [  92., 1495.,  526., 1768.],\n",
      "        [ 267.,   62.,  348.,  101.],\n",
      "        [ 581.,  306.,  640.,  345.],\n",
      "        [ 809.,  304., 1084.,  354.],\n",
      "        [ 765.,  573., 1128.,  623.],\n",
      "        [ 817., 1276.,  990., 1315.],\n",
      "        [ 140.,  604.,  470.,  679.],\n",
      "        [ 134.,  229.,  470.,  351.],\n",
      "        [ 212.,  934.,  431., 1015.],\n",
      "        [ 142., 1254.,  470., 1356.],\n",
      "        [ 120., 1584.,  487., 1656.],\n",
      "        [ 287.,  112.,  323.,  201.],\n",
      "        [ 292.,  451.,  320.,  531.],\n",
      "        [ 606.,  640.,  731.,  668.],\n",
      "        [ 290.,  770.,  320.,  868.],\n",
      "        [ 290., 1090.,  317., 1176.],\n",
      "        [ 290., 1418.,  320., 1506.],\n",
      "        [ 501., 1368.,  920., 1640.],\n",
      "        [ 487.,  770.,  970.,  993.]]), 'labels': tensor([7, 6, 6, 6, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2]), 'image_id': 0, 'area': [19412, 95120, 110352, 33201, 108407, 114675, 96775, 114863, 118482, 3159, 2301, 13750, 18150, 6747, 24750, 40992, 17739, 33456, 26424, 3204, 2240, 3500, 2940, 2322, 2640, 113968, 107709]})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\Documents\\SUTD Term 7\\50.035-cv-project\\faster_swin_t\\../dataset\\dataset_parser.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['boxes'] = torch.tensor(target['boxes'], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    \"text\" : 1,\n",
    "    \"arrow\" : 2,\n",
    "    \"connection\" : 3,\n",
    "    \"data\": 4,\n",
    "    \"decision\": 5,\n",
    "    \"process\" : 6,\n",
    "    \"terminator\" : 7\n",
    "}\n",
    "\n",
    "train = XMLDataset(image_dir='../dataset/train_img', annotation_dir='../dataset/xml_files/train', label_map=label_map, transform=transform)\n",
    "test = XMLDataset(image_dir='../dataset/test_img', annotation_dir='../dataset/xml_files/test', label_map=label_map, transform=transform)\n",
    "validation = XMLDataset(image_dir='../dataset/val_img', annotation_dir='../dataset/xml_files/val', label_map=label_map, transform=transform)\n",
    "data = train[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertions passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# assert data['objects']['bbox'][0].tolist() == [201., 31., 211., 92.], \"First box coordinates do not match\"\n",
    "(image, target) = data\n",
    "# assert target['labels'][:5] == [7, 6, 6, 6, 5], \"First five labels do not match\"\n",
    "assert len(target['area']) == len(target['labels']), \"Length of area and category do not match\"\n",
    "assert len(target['boxes']) == len(target['area']), \"Length of bbox and area do not match\"\n",
    "# print(\"new image size\", image.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Assertions passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FCA dataset (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomSwinBackbone(\n",
      "  (backbone): Swinv2Backbone(\n",
      "    (embeddings): Swinv2Embeddings(\n",
      "      (patch_embeddings): Swinv2PatchEmbeddings(\n",
      "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Swinv2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=False)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-5): 6 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, Swinv2Backbone, AutoModel, Swinv2Config, AutoBackbone\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# config = Swinv2Config()\n",
    "# config.out_features = [\"stage2\", \"stage3\", \"stage4\"]\n",
    "# swin_backbone = AutoModel.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "swin_backbone = AutoBackbone.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\", out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n",
    "\n",
    "# print(config)\n",
    "\n",
    "\n",
    "class CustomSwinBackbone(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone\n",
    "        out = self.backbone(x, output_hidden_states=False, output_attentions=False)\n",
    "        feature_map = out.feature_maps\n",
    "        print(len(out.feature_maps))\n",
    "        out_channel = 768\n",
    "        feature_dict = {}\n",
    "        \n",
    "        for i in range(len(feature_map)):\n",
    "            in_channel = list(out.feature_maps[i].shape)[1]\n",
    "            conv = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1)\n",
    "            feature_dict[str(i)] = conv(feature_map[i])\n",
    "            \n",
    "        print(feature_dict[\"0\"].shape)\n",
    "        print(feature_dict[\"1\"].shape)\n",
    "        print(feature_dict[\"2\"].shape)\n",
    "        print(feature_dict[\"3\"].shape)\n",
    "\n",
    "        # Permute the output to (b, c, h, w)\n",
    "        # out[0] corresponds to the feature map (assuming the backbone outputs a list of feature maps)\n",
    "        # out[0].shape is (b, h, w, c), and we need to permute it to (b, c, h, w)\n",
    "        return feature_dict\n",
    "# Create the custom backbone with the permute operation\n",
    "swin_backbone = CustomSwinBackbone(swin_backbone)\n",
    "print(swin_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision\n",
    "\n",
    "swin_backbone.out_channels = 768\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),\n",
    "           (32, 64, 128, 256, 512),\n",
    "           (32, 64, 128, 256, 512),\n",
    "           (32, 64, 128, 256, 512)), \n",
    "    aspect_ratios=((0.5, 1.0, 2.0),\n",
    "                   (0.5, 1.0, 2.0),\n",
    "                   (0.5, 1.0, 2.0),\n",
    "                   (0.5, 1.0, 2.0)))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=[\"1\", \"2\", \"3\"], \n",
    "    output_size = 7, \n",
    "    sampling_ratio=2\n",
    "    )\n",
    "\n",
    "model = FasterRCNN(\n",
    "    backbone=swin_backbone,\n",
    "    #7 + 1 for background\n",
    "    num_classes=8,\n",
    "    # min_size=256,\n",
    "    # max_size=256,\n",
    "    # image_mean=mean,\n",
    "    # image_std=std,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9362358181861065, 0.9420508144749817, 0.9394349808120092] [0.16755679634617304, 0.15510376581525342, 0.1576581799256157]\n"
     ]
    }
   ],
   "source": [
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    validation,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edited from pytorch documentation\n",
    "def train_one_epoch(model, optimizer, data_loader,  epoch, lr_scheduler):\n",
    "    try: \n",
    "        model.train()\n",
    "        header = f\"Epoch: [{epoch}]\"\n",
    "        running_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(data_loader):\n",
    "            images, labels = data\n",
    "            \n",
    "            loss_dict = model(images, labels)\n",
    "            print(loss_dict)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # Gather data and report\n",
    "            running_loss += losses.item()\n",
    "            if i % 1000 == 4:\n",
    "                last_loss = running_loss / 4 # loss per batch\n",
    "                print(header)\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "    except Exception as e:\n",
    "        # Print the error and the image_id that caused it\n",
    "        # print(f\"Error for image name {labels['image_id']}\")\n",
    "        # print(data)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        # You can return None or raise the error depending on your need\n",
    "        raise e\n",
    "        \n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "4\n",
      "torch.Size([4, 768, 336, 296])\n",
      "torch.Size([4, 768, 168, 148])\n",
      "torch.Size([4, 768, 84, 74])\n",
      "torch.Size([4, 768, 42, 37])\n",
      "{'loss_classifier': tensor(1.9869, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0809, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.6825, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.4134, grad_fn=<DivBackward0>)}\n",
      "4\n",
      "torch.Size([4, 768, 312, 336])\n",
      "torch.Size([4, 768, 156, 168])\n",
      "torch.Size([4, 768, 78, 84])\n",
      "torch.Size([4, 768, 39, 42])\n",
      "{'loss_classifier': tensor(1.8923, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.2611, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.6814, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.3178, grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch= epoch_number, model=model, data_loader=data_loader, optimizer=optimizer, lr_scheduler=lr_scheduler)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        running_vloss = 0.0\n",
    "        total_predictions = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i, vdata in enumerate(data_loader_val):\n",
    "            vinputs, vlabels = vdata  # Images and their ground-truth labels\n",
    "            \n",
    "            # Compute the model outputs and loss\n",
    "            loss_dict = model(vinputs, vlabels)  # Pass inputs and targets\n",
    "            vloss = sum(loss for loss in loss_dict.values())\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            # Perform inference to calculate accuracy\n",
    "            predictions = model(vinputs)  # Inference mode\n",
    "            for j, prediction in enumerate(predictions):\n",
    "                gt_boxes = vlabels[j]['boxes']  # Ground-truth boxes\n",
    "                gt_labels = vlabels[j]['labels']  # Ground-truth labels\n",
    "\n",
    "                # Match predictions to ground truth (e.g., IoU thresholding)\n",
    "                # Increment `correct_predictions` and `total_predictions` based on matches\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "        print(f\"LOSS train {avg_loss} valid {avg_vloss}\")\n",
    "        print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
