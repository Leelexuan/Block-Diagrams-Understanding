{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CBD dataset (for training)\n",
    "\n",
    "### Transformation\n",
    " - resize to 244,244\n",
    " - normalized based on calculations from training set\n",
    " - converted to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../dataset')\n",
    "from dataset_parser import XMLDataset\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.9362358181861065, 0.9420508144749817, 0.9394349808120092]\n",
      "Std: [0.16755679634617304, 0.15510376581525342, 0.1576581799256157]\n"
     ]
    }
   ],
   "source": [
    "#calculation mean and std of training set for normalization\n",
    "\n",
    "def compute_dataset_stats(image_dir):\n",
    "    mean = np.zeros(3)\n",
    "    std = np.zeros(3)\n",
    "    total_pixels = 0\n",
    "\n",
    "    # List all files in the directory\n",
    "    image_filenames = sorted(os.listdir(image_dir))\n",
    "\n",
    "    for img_filename in image_filenames:\n",
    "        img_path = os.path.join(image_dir, img_filename)\n",
    "\n",
    "        # Open the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image) / 255.0  # Convert to range [0, 1]\n",
    "\n",
    "        # Compute mean and std\n",
    "        mean += image.mean(axis=(0, 1))\n",
    "        std += image.std(axis=(0, 1))\n",
    "        total_pixels += 1\n",
    "\n",
    "    # Compute average mean and std across all images\n",
    "    mean /= total_pixels\n",
    "    std /= total_pixels\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "# Example usage\n",
    "image_dir = '../dataset/train_img'\n",
    "mean, std = compute_dataset_stats(image_dir)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#apply transformations to image and bounding boxes\n",
    "\n",
    "transform = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "    ], bbox_params=albumentations.BboxParams(format='pascal_voc',  label_fields=['category']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0763, 0.0763, 0.0763,  ..., 0.0763, 0.0763, 0.0763],\n",
      "         [0.0763, 0.0763, 0.0763,  ..., 0.0763, 0.0763, 0.0763],\n",
      "         [0.0763, 0.0763, 0.0763,  ..., 0.0763, 0.0763, 0.0763],\n",
      "         ...,\n",
      "         [0.0763, 0.0763, 0.0763,  ..., 0.0763, 0.0763, 0.0763],\n",
      "         [0.0763, 0.0763, 0.0763,  ..., 0.0763, 0.0763, 0.0763],\n",
      "         [0.0763, 0.0763, 0.0763,  ..., 0.0763, 0.0763, 0.0763]],\n",
      "\n",
      "        [[0.0449, 0.0449, 0.0449,  ..., 0.0449, 0.0449, 0.0449],\n",
      "         [0.0449, 0.0449, 0.0449,  ..., 0.0449, 0.0449, 0.0449],\n",
      "         [0.0449, 0.0449, 0.0449,  ..., 0.0449, 0.0449, 0.0449],\n",
      "         ...,\n",
      "         [0.0449, 0.0449, 0.0449,  ..., 0.0449, 0.0449, 0.0449],\n",
      "         [0.0449, 0.0449, 0.0449,  ..., 0.0449, 0.0449, 0.0449],\n",
      "         [0.0449, 0.0449, 0.0449,  ..., 0.0449, 0.0449, 0.0449]],\n",
      "\n",
      "        [[0.0608, 0.0608, 0.0608,  ..., 0.0608, 0.0608, 0.0608],\n",
      "         [0.0608, 0.0608, 0.0608,  ..., 0.0608, 0.0608, 0.0608],\n",
      "         [0.0608, 0.0608, 0.0608,  ..., 0.0608, 0.0608, 0.0608],\n",
      "         ...,\n",
      "         [0.0608, 0.0608, 0.0608,  ..., 0.0608, 0.0608, 0.0608],\n",
      "         [0.0608, 0.0608, 0.0608,  ..., 0.0608, 0.0608, 0.0608],\n",
      "         [0.0608, 0.0608, 0.0608,  ..., 0.0608, 0.0608, 0.0608]]]), {'boxes': tensor([[ 36.9959,   3.8301,  75.8324,  15.1969],\n",
      "        [132.5226,  27.9228, 217.9260,  53.2510],\n",
      "        [132.5226,  65.9768, 216.4536,  95.8764],\n",
      "        [146.8792, 150.7336, 186.8200, 169.6371],\n",
      "        [ 18.9581,  23.1042,  93.1339,  56.3398],\n",
      "        [ 17.4856,  63.6293,  94.2383,  97.6062],\n",
      "        [ 19.5103, 106.1313,  92.2136, 136.4016],\n",
      "        [ 16.5653, 143.9382,  95.1586, 177.1738],\n",
      "        [ 16.9334, 184.7104,  96.8151, 218.4402],\n",
      "        [ 49.1438,   7.6602,  64.0526,  12.4788],\n",
      "        [106.9384,  37.8069, 117.7979,  42.6255],\n",
      "        [148.9039,  37.5598, 199.5201,  43.7375],\n",
      "        [140.8053,  70.7954, 207.6187,  76.9730],\n",
      "        [150.3763, 157.6525, 182.2186, 162.4711],\n",
      "        [ 25.7683,  74.6255,  86.5078,  83.8919],\n",
      "        [ 24.6639,  28.2934,  86.5078,  43.3668],\n",
      "        [ 39.0205, 115.3977,  79.3295, 125.4054],\n",
      "        [ 26.1364, 154.9344,  86.5078, 167.5367],\n",
      "        [ 22.0871, 195.7066,  89.6368, 204.6023],\n",
      "        [ 52.8250,  13.8378,  59.4511,  24.8340],\n",
      "        [ 53.7453,  55.7220,  58.8989,  65.6062],\n",
      "        [111.5398,  79.0734, 134.5472,  82.5328],\n",
      "        [ 53.3772,  95.1351,  58.8989, 107.2432],\n",
      "        [ 53.3772, 134.6718,  58.3468, 145.2973],\n",
      "        [ 53.3772, 175.1969,  58.8989, 186.0695],\n",
      "        [ 92.2136, 169.0193, 169.3344, 202.6255],\n",
      "        [ 89.6368,  95.1351, 178.5374, 122.6873]]), 'labels': tensor([7, 6, 6, 6, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2]), 'image_id': 0, 'area': [19412, 95120, 110352, 33201, 108407, 114675, 96775, 114863, 118482, 3159, 2301, 13750, 18150, 6747, 24750, 40992, 17739, 33456, 26424, 3204, 2240, 3500, 2940, 2322, 2640, 113968, 107709]})\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    \"text\" : 1,\n",
    "    \"arrow\" : 2,\n",
    "    \"connection\" : 3,\n",
    "    \"data\": 4,\n",
    "    \"decision\": 5,\n",
    "    \"process\" : 6,\n",
    "    \"terminator\" : 7\n",
    "}\n",
    "\n",
    "train = XMLDataset(image_dir='../dataset/train_img', annotation_dir='../dataset/xml_files/train', label_map=label_map, transform=transform)\n",
    "test = XMLDataset(image_dir='../dataset/test_img', annotation_dir='../dataset/xml_files/test', label_map=label_map, transform=transform)\n",
    "validation = XMLDataset(image_dir='../dataset/val_img', annotation_dir='../dataset/xml_files/val', label_map=label_map, transform=transform)\n",
    "data = train[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertions passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# assert data['objects']['bbox'][0].tolist() == [201., 31., 211., 92.], \"First box coordinates do not match\"\n",
    "(image, target) = data\n",
    "# assert target['labels'][:5] == [7, 6, 6, 6, 5], \"First five labels do not match\"\n",
    "assert len(target['area']) == len(target['labels']), \"Length of area and category do not match\"\n",
    "assert len(target['boxes']) == len(target['area']), \"Length of bbox and area do not match\"\n",
    "# print(\"new image size\", image.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Assertions passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FCA dataset (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "# auto image processor does auto image transformation + normalization \n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")\n",
    "swin_backbone = AutoFeatureExtractor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision\n",
    "\n",
    "swin_backbone.out_channels = 768\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size = 7, sampling_ratio=2)\n",
    "model = FasterRCNN(\n",
    "    backbone=swin_backbone,\n",
    "    #7 + 1 for background\n",
    "    num_classes=8,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     images, targets = zip(*batch)\n",
    "    \n",
    "#     # Pad the bounding boxes and labels to the same length\n",
    "#     max_boxes = max(len(target['boxes']) for target in targets)\n",
    "    \n",
    "#     for target in targets:\n",
    "#         num_boxes = len(target['boxes'])\n",
    "#         padding = max_boxes - num_boxes\n",
    "#         if padding > 0:\n",
    "#             target['boxes'] = torch.cat([target['boxes'], torch.zeros(padding, 4)], dim=0)\n",
    "#             target['labels'] = torch.cat([target['labels'], torch.zeros(padding)], dim=0)  # Assuming labels are integer values\n",
    "    \n",
    "#     return default_collate(images), targets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    validation,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edited from pytorch documentation\n",
    "def train_one_epoch(model, optimizer, data_loader,  epoch, lr_scheduler):\n",
    "    try: \n",
    "        model.train()\n",
    "        header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "            images, labels = data\n",
    "            \n",
    "            loss_dict = model(images, labels)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # Gather data and report\n",
    "            running_loss += losses.item()\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000 # loss per batch\n",
    "                print(header)\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                running_loss = 0.\n",
    "    except Exception as e:\n",
    "        # Print the error and the image_id that caused it\n",
    "        # print(f\"Error for image name {labels['image_id']}\")\n",
    "        print(data)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        # You can return None or raise the error depending on your need\n",
    "        raise e\n",
    "        \n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "((tensor([[[0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         ...,\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806]],\n",
      "\n",
      "        [[0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         ...,\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736]],\n",
      "\n",
      "        [[0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         ...,\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842]]]), tensor([[[0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         ...,\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806]],\n",
      "\n",
      "        [[0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         ...,\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736]],\n",
      "\n",
      "        [[0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         ...,\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842]]]), tensor([[[0.3337, 0.3337, 0.3337,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3337, 0.3337, 0.3337,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3337, 0.3337, 0.3337,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         ...,\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806]],\n",
      "\n",
      "        [[0.3230, 0.3230, 0.3230,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3230, 0.3230, 0.3230,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3230, 0.3230, 0.3230,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         ...,\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736]],\n",
      "\n",
      "        [[0.3344, 0.3344, 0.3344,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3344, 0.3344, 0.3344,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3344, 0.3344, 0.3344,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         ...,\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842]]]), tensor([[[0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         ...,\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806]],\n",
      "\n",
      "        [[0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         ...,\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736]],\n",
      "\n",
      "        [[0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         ...,\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842]]])), ({'boxes': tensor([[ 84.5935,   1.6402, 127.7534,  16.1674],\n",
      "        [ 90.6358,  36.0837, 125.1638,  41.2385],\n",
      "        [ 86.7514,  66.5439, 129.0482,  75.9163],\n",
      "        [170.4817,  80.1339, 204.1464,  90.2092],\n",
      "        [ 15.1060,  78.7280,  60.8555,  92.3180],\n",
      "        [ 83.2986, 103.0962, 135.9538, 112.9372],\n",
      "        [ 81.5723, 141.5230, 126.4586, 152.0670],\n",
      "        [ 85.8882, 173.3891, 127.7534, 182.7615],\n",
      "        [ 82.0039, 204.0837, 121.7110, 209.9414],\n",
      "        [ 78.1195,   0.7029, 133.3642,  18.7448],\n",
      "        [ 79.4143,  30.2259, 134.2274,  47.7992],\n",
      "        [ 78.9827,  63.7322, 133.3642,  79.4310],\n",
      "        [152.3545,  75.4477, 212.3468,  95.1297],\n",
      "        [  1.7264,  75.4477,  65.1715,  96.7699],\n",
      "        [ 68.1927,  97.7071, 141.1329, 118.7950],\n",
      "        [ 73.3719, 136.1339, 139.4066, 155.8159],\n",
      "        [ 74.6667, 169.4059, 136.8170, 186.2762],\n",
      "        [ 72.5087, 201.5063, 134.6590, 218.6109],\n",
      "        [ 96.2466,  16.4017, 115.2370,  31.3975],\n",
      "        [ 95.8150,  47.7992, 115.6686,  63.9665],\n",
      "        [125.1638,  72.1674, 161.4181,  88.1004],\n",
      "        [ 53.5183,  72.8703,  87.1830,  88.5690],\n",
      "        [ 98.4046,  79.4310, 117.3950,  98.4100],\n",
      "        [ 96.6782, 117.6234, 114.8054, 137.0711],\n",
      "        [ 95.3834, 153.2385, 116.1002, 171.0460],\n",
      "        [ 97.5414, 185.5732, 113.9422, 201.9749]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2]), 'image_id': 173, 'area': [6200, 1760, 3920, 3354, 6148, 5124, 4680, 3880, 2300, 9856, 9525, 8442, 11676, 13377, 15210, 12852, 10368, 10512, 2816, 3174, 5712, 5226, 3564, 3486, 3648, 2660]}, {'boxes': tensor([[ 24.6578,  28.1379,  32.9761,  34.2069],\n",
      "        [ 60.9019,  28.6897,  68.9231,  34.7586],\n",
      "        [ 93.2838,  28.1379, 100.4138,  35.3103],\n",
      "        [125.3687,  29.7931, 132.4987,  35.8621],\n",
      "        [157.1565,  29.2414, 166.3661,  35.8621],\n",
      "        [188.9443,  29.2414, 206.1751,  83.3103],\n",
      "        [194.5889, 108.1379, 205.8780, 171.0345],\n",
      "        [157.4536, 161.1035, 166.0690, 169.3793],\n",
      "        [125.0716, 161.6552, 133.0928, 168.2759],\n",
      "        [ 94.1751, 160.5517, 101.6021, 168.2759],\n",
      "        [ 59.1194, 161.1035,  67.1406, 171.0345],\n",
      "        [ 24.9549, 163.8621,  34.4615, 171.5862],\n",
      "        [  7.1300,  18.2069,  19.6074,  35.3103],\n",
      "        [ 35.9470,  16.5517,  57.6339,  36.9655],\n",
      "        [ 68.0318,  24.8276,  91.7984,  34.2069],\n",
      "        [105.7613,  17.6552, 120.3183,  37.5172],\n",
      "        [137.5491,  29.2414, 152.1061,  37.5172],\n",
      "        [168.4456,   9.3793, 187.1618,  32.0000],\n",
      "        [196.3714,  86.6207, 212.1167,  93.7931],\n",
      "        [167.5544, 142.3448, 192.8064, 162.7586],\n",
      "        [136.3607, 161.6552, 152.1061, 170.4828],\n",
      "        [104.8700, 151.1724, 120.9125, 171.0345],\n",
      "        [ 68.3289, 155.5862,  91.5013, 176.5517],\n",
      "        [ 35.0557, 152.2759,  57.6339, 170.4828],\n",
      "        [  7.4271, 158.8965,  21.3899, 175.4483],\n",
      "        [  0.2971,  11.5862,  26.7374,  50.7586],\n",
      "        [ 32.3820,  11.5862,  60.9019,  51.3103],\n",
      "        [ 67.1406,  12.6897,  94.1751,  50.7586],\n",
      "        [ 99.2255,  13.2414, 126.8541,  51.3103],\n",
      "        [131.3103,  13.7931, 158.6419,  50.7586],\n",
      "        [164.2865,   3.8621, 191.6180,  56.2759],\n",
      "        [193.1034,  80.5517, 215.9788, 111.4483],\n",
      "        [164.2865, 135.7241, 196.3714, 190.3448],\n",
      "        [131.3103, 145.1035, 158.6419, 183.7241],\n",
      "        [ 65.0610, 146.2069,  96.8488, 183.7241],\n",
      "        [ 98.6313, 145.1035, 127.1512, 183.7241],\n",
      "        [ 32.3820, 146.7586,  61.4960, 185.9310],\n",
      "        [  0.5942, 151.7241,  26.7374, 180.4138]]), 'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), 'image_id': 206, 'area': [308, 297, 312, 264, 372, 5684, 4332, 435, 324, 350, 486, 448, 1302, 2701, 1360, 1764, 735, 2583, 689, 3145, 848, 1944, 2964, 2508, 1410, 6319, 6912, 6279, 6417, 6164, 8740, 4312, 10692, 6440, 7276, 6720, 6958, 4576]}, {'boxes': tensor([[ 91.4023,  21.5385, 101.7011,  39.6308],\n",
      "        [ 91.4023,  63.7538, 102.9885,  80.1231],\n",
      "        [ 91.4023, 100.8000, 101.7011, 117.1692],\n",
      "        [ 90.1149, 155.0769, 101.7011, 171.4462],\n",
      "        [ 79.8161,   7.7538, 122.2988,  16.3692],\n",
      "        [ 46.3448,  47.3846, 155.7701,  57.7231],\n",
      "        [ 37.3333,  82.7077, 142.8966,  93.0462],\n",
      "        [ 34.7586, 130.9539, 113.2874, 152.4923],\n",
      "        [ 82.3908, 179.2000, 115.8621, 189.5385],\n",
      "        [ 29.6092,   0.8615, 173.7931,  23.2615],\n",
      "        [ 28.3218, 171.4462, 171.2184, 194.7077],\n",
      "        [  1.2874, 115.4462, 220.1379, 159.3846],\n",
      "        [ 15.4483,  37.0462, 180.2299,  66.3385],\n",
      "        [ 29.6092,  75.8154, 169.9310, 103.3846]]), 'labels': tensor([2, 2, 2, 2, 1, 1, 1, 1, 1, 7, 7, 4, 4, 6]), 'image_id': 142, 'area': [168, 171, 152, 171, 330, 1020, 984, 1525, 312, 2912, 2997, 8670, 4352, 3488]}, {'boxes': tensor([[ 40.6588,   5.6530, 100.8941,  25.6740],\n",
      "        [135.5294, 165.8212, 195.3882, 185.6067],\n",
      "        [ 38.7765, 187.2555,  99.3882, 207.7476],\n",
      "        [ 38.7765, 153.1020,  99.3882, 173.5941],\n",
      "        [ 38.7765,  66.1872, 100.1412,  86.9148],\n",
      "        [124.6118, 130.0189, 208.1882, 149.8044],\n",
      "        [ 28.9882,  36.7445, 111.4353,  56.5300],\n",
      "        [ 34.6353,  98.4564, 105.0353, 140.3828],\n",
      "        [ 62.8706,  12.7192,  77.5529,  18.6078],\n",
      "        [ 45.9294,  43.8107,  93.3647,  49.2282],\n",
      "        [ 52.3294,  71.8402,  88.4706,  78.6709],\n",
      "        [113.6941, 114.4732, 123.4824, 118.7129],\n",
      "        [158.8706, 173.5941, 172.0471, 178.7760],\n",
      "        [136.6588, 137.0852, 196.1412, 143.9159],\n",
      "        [ 69.2706, 141.7960,  78.6824, 146.2713],\n",
      "        [ 43.6706, 117.5352,  96.0000, 123.4238],\n",
      "        [ 49.3176, 159.9327,  87.3412, 166.7634],\n",
      "        [ 53.0824, 194.0862,  86.2118, 200.9169],\n",
      "        [ 66.6353,  23.7897,  73.4118,  37.6866],\n",
      "        [ 66.6353,  53.7035,  72.6588,  67.3649],\n",
      "        [ 66.6353,  84.7950,  72.2824,  99.8696],\n",
      "        [101.6471, 118.2419, 168.6588, 131.1966],\n",
      "        [162.2588, 148.6267, 168.2823, 167.7056],\n",
      "        [ 65.5059, 171.9453,  72.2824, 188.9043],\n",
      "        [ 65.5059, 138.7340,  72.2824, 154.5152],\n",
      "        [ 16.5647,  97.7497,  71.5294, 214.1073]]), 'labels': tensor([7, 7, 6, 6, 6, 4, 4, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2]), 'image_id': 221, 'area': [13600, 13356, 14007, 14007, 14344, 18648, 18396, 33286, 975, 2898, 2784, 468, 770, 4582, 475, 3475, 2929, 2552, 1062, 928, 960, 9790, 1296, 1296, 1206, 72124]}))\n",
      "Error: The image to be converted to a PIL image contains values outside the range [0, 1], got [-27.73265838623047, -0.09709371626377106] which cannot be converted to uint8.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-27.73265838623047, -0.09709371626377106] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 33\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, epoch, lr_scheduler)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# You can return None or raise the error depending on your need\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m last_loss\n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, epoch, lr_scheduler)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m      8\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 10\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\transformers\\image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\transformers\\utils\\generic.py:852\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    845\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:251\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[0;32m    247\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(images[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m    250\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[0;32m    253\u001b[0m     ]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[0;32m    256\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[0;32m    259\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:138\u001b[0m, in \u001b[0;36mViTImageProcessor.resize\u001b[1;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m output_size \u001b[38;5;241m=\u001b[39m (size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\transformers\\image_transforms.py:337\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[0;32m    335\u001b[0m do_rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m--> 337\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m \u001b[43m_rescale_for_pil_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_pil_image(image, do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[0;32m    339\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n",
      "File \u001b[1;32mc:\\Users\\Lee Le Xuan\\anaconda3\\envs\\ComputerVision2\\Lib\\site-packages\\transformers\\image_transforms.py:158\u001b[0m, in \u001b[0;36m_rescale_for_pil_conversion\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    156\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] which cannot be converted to uint8.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m do_rescale\n",
      "\u001b[1;31mValueError\u001b[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-27.73265838623047, -0.09709371626377106] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch= epoch_number, model=model, data_loader=data_loader, optimizer=optimizer, lr_scheduler=lr_scheduler)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(data_loader_val):\n",
    "            vinputs, vlabels = vdata\n",
    "            print(vdata)\n",
    "            loss_dict = model(vinputs, vlabels)\n",
    "            vloss = sum(loss for loss in loss_dict.values())\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         ...,\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806],\n",
      "         [0.3806, 0.3806, 0.3806,  ..., 0.3806, 0.3806, 0.3806]],\n",
      "\n",
      "        [[0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         ...,\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736],\n",
      "         [0.3736, 0.3736, 0.3736,  ..., 0.3736, 0.3736, 0.3736]],\n",
      "\n",
      "        [[0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         ...,\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842],\n",
      "         [0.3842, 0.3842, 0.3842,  ..., 0.3842, 0.3842, 0.3842]]]), {'boxes': tensor([[ 13.1250,  23.7464,  46.2000,  40.8831],\n",
      "        [ 68.7750,  22.2776,  97.4750,  43.8208],\n",
      "        [120.5750,  22.0328, 151.2000,  45.5344],\n",
      "        [173.0750,  22.2776, 201.9500,  44.8000],\n",
      "        [171.8500,  73.4426, 201.7750,  95.4754],\n",
      "        [120.5750,  73.6874, 150.6750,  95.2306],\n",
      "        [ 68.0750,  73.4426,  98.8750,  95.4754],\n",
      "        [125.3000, 127.5453, 154.7000, 148.1093],\n",
      "        [ 66.1500, 176.9967, 100.6250, 203.1913],\n",
      "        [ 58.4500, 115.3049, 107.4500, 158.3913],\n",
      "        [ 45.3250,  28.1530,  68.4250,  37.7005],\n",
      "        [ 96.7750,  29.6219, 123.9000,  36.2317],\n",
      "        [148.7500,  28.1530, 173.2500,  36.9661],\n",
      "        [183.2250,  43.0863, 190.0500,  74.1770],\n",
      "        [148.9250,  80.2973, 173.2500,  89.6000],\n",
      "        [ 97.1250,  80.5421, 122.1500,  88.1311],\n",
      "        [ 79.6250,  94.7410,  86.2750, 117.2634],\n",
      "        [ 22.9250,  28.3978,  35.3500,  35.7421],\n",
      "        [ 74.3750,  26.1945,  92.7500,  40.1486],\n",
      "        [126.1750,  27.4186, 144.3750,  37.9454],\n",
      "        [176.5750,  26.4393, 199.3250,  40.3934],\n",
      "        [176.2250,  80.5421, 197.5750,  89.6000],\n",
      "        [128.1000,  80.7869, 147.1750,  89.1104],\n",
      "        [ 73.3250,  77.3596,  93.8000,  91.3137],\n",
      "        [ 68.7750, 132.4415,  99.0500, 149.5781],\n",
      "        [131.2500, 135.3792, 150.6750, 142.7235],\n",
      "        [ 69.4750, 178.7104,  98.7000, 200.9880],\n",
      "        [107.1000, 133.6656, 125.3000, 141.7443],\n",
      "        [112.7000, 134.6448, 120.5750, 140.2754],\n",
      "        [ 79.4500, 156.6776,  85.9250, 177.4863],\n",
      "        [ 79.9750, 164.7563,  85.5750, 170.3869]]), 'labels': tensor([7, 6, 6, 6, 6, 6, 6, 6, 6, 5, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 2, 1]), 'image_id': 0, 'area': [13230, 14432, 16800, 15180, 15390, 15136, 15840, 14112, 21079, 49280, 5148, 4185, 5040, 4953, 5282, 4433, 3496, 2130, 5985, 4472, 7410, 4514, 3706, 6669, 12110, 3330, 15197, 3432, 1035, 3145, 736]})\n"
     ]
    }
   ],
   "source": [
    "print(validation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
